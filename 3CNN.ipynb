{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "486946f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7078cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cdd6b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 3D CNN model for collision prediction\n",
    "class CollisionPredictor3DCNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, clip_len=16, height=112, width=112):\n",
    "        super(CollisionPredictor3DCNN, self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.clip_len = clip_len\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        print(f\"Model initialized with input_channels={input_channels}, clip_len={clip_len}, height={height}, width={width}\")\n",
    "        \n",
    "        # First 3D convolutional block\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(input_channels, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        )\n",
    "        \n",
    "        # Second 3D convolutional block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        )\n",
    "        \n",
    "        # Third 3D convolutional block\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        )\n",
    "        \n",
    "        # Fourth 3D convolutional block\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        )\n",
    "        \n",
    "        # Calculate size of flattened features after convolutions\n",
    "        c_len, h, w = self._calculate_conv_output_size()\n",
    "        self.flat_size = 512 * c_len * h * w\n",
    "        print(f\"Calculated flat size: {self.flat_size} (512 * {c_len} * {h} * {w})\")\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(self.flat_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Additional input for time features (time_of_event, time_of_alert)\n",
    "        self.time_fc = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024 + 64, 512),  # Combine video features with time features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Output layer (collision probability)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def _calculate_conv_output_size(self):\n",
    "        # Calculate output size after convolutional layers\n",
    "        c_len, h, w = self.clip_len, self.height, self.width\n",
    "        \n",
    "        # After conv1: [clip_len, height/2, width/2]\n",
    "        c_len, h, w = c_len, h // 2, w // 2\n",
    "        \n",
    "        # After conv2: [clip_len, height/4, width/4]\n",
    "        c_len, h, w = c_len, h // 2, w // 2\n",
    "        \n",
    "        # After conv3: [clip_len/2, height/8, width/8]\n",
    "        c_len, h, w = c_len // 2, h // 2, w // 2\n",
    "        \n",
    "        # After conv4: [clip_len/4, height/16, width/16]\n",
    "        c_len, h, w = c_len // 2, h // 2, w // 2\n",
    "        \n",
    "        return c_len, h, w\n",
    "    \n",
    "    def forward(self, x, time_features=None):\n",
    "        # Check input shape and rearrange if needed\n",
    "        # Expected shape: [batch_size, channels, clip_len, height, width]\n",
    "        # If shape is [batch_size, height, clip_len, channels, width], we need to permute\n",
    "        \n",
    "        if x.shape[1] > 5:  # This suggests height is in channel position\n",
    "            # Permute to get [batch_size, channels, clip_len, height, width]\n",
    "            x = x.permute(0, 3, 2, 1, 4)\n",
    "            print(f\"Permuted input shape: {x.shape}\")\n",
    "        \n",
    "        # Apply convolutional blocks\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        if time_features is not None:\n",
    "            # Process time features\n",
    "            time_feats = self.time_fc(time_features)\n",
    "            \n",
    "            # Concatenate with CNN features\n",
    "            x = torch.cat((x, time_feats), dim=1)\n",
    "        else:\n",
    "            # If no time features, pad with zeros\n",
    "            batch_size = x.size(0)\n",
    "            x = torch.cat((x, torch.zeros(batch_size, 64, device=x.device)), dim=1)\n",
    "        \n",
    "        # Apply second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57e02f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for video collision data\n",
    "class VideoCollisionDataset(Dataset):\n",
    "    def __init__(self, video_paths, time_of_events, time_of_alerts, targets, \n",
    "                 clip_len=16, spatial_size=(112, 112), transform=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.time_of_events = time_of_events\n",
    "        self.time_of_alerts = time_of_alerts\n",
    "        self.targets = targets\n",
    "        self.clip_len = clip_len\n",
    "        self.spatial_size = spatial_size\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load video\n",
    "        video = self.load_video(self.video_paths[idx])\n",
    "        \n",
    "        # Extract clip from video\n",
    "        clip = self.extract_clip(video, idx)\n",
    "        \n",
    "        # Prepare time features\n",
    "        time_features = self.prepare_time_features(idx)\n",
    "        \n",
    "        # If transform exists, apply it to each frame\n",
    "        if self.transform:\n",
    "            transformed_clip = []\n",
    "            for frame in clip:\n",
    "                # frame is already a numpy array (H, W, C), so apply transform directly\n",
    "                frame_tensor = self.transform(frame)  # Results in tensor (C, H, W)\n",
    "                transformed_clip.append(frame_tensor)\n",
    "            \n",
    "            # Stack along time dimension\n",
    "            clip = torch.stack(transformed_clip)  # Shape (T, C, H, W)\n",
    "            # Permute to get (C, T, H, W) for 3D CNN\n",
    "            clip = clip.permute(1, 0, 2, 3)\n",
    "        else:\n",
    "            # If no transform, manually handle the conversion\n",
    "            # Convert from (T, H, W, C) to (C, T, H, W)\n",
    "            clip = np.transpose(clip, (3, 0, 1, 2))\n",
    "            clip = torch.from_numpy(clip).float()\n",
    "        \n",
    "        target = torch.tensor([self.targets[idx]], dtype=torch.float)\n",
    "        time_features = torch.from_numpy(time_features).float()\n",
    "        \n",
    "        return clip, time_features, target\n",
    "    \n",
    "    def load_video(self, video_path):\n",
    "        \"\"\"\n",
    "        Load video from file.\n",
    "        \"\"\"\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Resize frame\n",
    "            frame = cv2.resize(frame, self.spatial_size)\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if not frames:\n",
    "            raise ValueError(f\"No frames could be read from video: {video_path}\")\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        video = np.array(frames)\n",
    "        \n",
    "        return video\n",
    "    \n",
    "    def extract_clip(self, video, idx):\n",
    "        \"\"\"\n",
    "        Extract clip from video based on target and time information.\n",
    "        \"\"\"\n",
    "        if self.targets[idx] == 1 and self.time_of_events[idx] is not None:\n",
    "            # If there's a collision, center the clip around the collision time\n",
    "            center_frame = self.time_to_frame(self.time_of_events[idx], video.shape[0])\n",
    "            start_frame = max(0, center_frame - self.clip_len // 2)\n",
    "            end_frame = min(video.shape[0], start_frame + self.clip_len)\n",
    "            \n",
    "            # If we're at the edge of the video, adjust start_frame\n",
    "            if end_frame - start_frame < self.clip_len:\n",
    "                start_frame = max(0, end_frame - self.clip_len)\n",
    "            \n",
    "            clip = video[start_frame:end_frame]\n",
    "            \n",
    "            # If clip is shorter than clip_len, pad it\n",
    "            if clip.shape[0] < self.clip_len:\n",
    "                padding = np.zeros((self.clip_len - clip.shape[0], *clip.shape[1:]), dtype=clip.dtype)\n",
    "                clip = np.concatenate([clip, padding], axis=0)\n",
    "        else:\n",
    "            # If no collision or no time_of_event, extract a random clip\n",
    "            if video.shape[0] <= self.clip_len:\n",
    "                # If video is shorter than clip_len, pad it\n",
    "                padding = np.zeros((self.clip_len - video.shape[0], *video.shape[1:]), dtype=video.dtype)\n",
    "                clip = np.concatenate([video, padding], axis=0)\n",
    "            else:\n",
    "                # Randomly select a starting point\n",
    "                start_frame = np.random.randint(0, video.shape[0] - self.clip_len + 1)\n",
    "                clip = video[start_frame:start_frame + self.clip_len]\n",
    "        \n",
    "        return clip\n",
    "    \n",
    "    def prepare_time_features(self, idx):\n",
    "        \"\"\"\n",
    "        Prepare time features (time_of_event, time_of_alert).\n",
    "        \"\"\"\n",
    "        # Normalize time values to [0, 1] or use -1 for None\n",
    "        time_event = -1 if self.time_of_events[idx] is None else self.time_of_events[idx]\n",
    "        time_alert = -1 if self.time_of_alerts[idx] is None else self.time_of_alerts[idx]\n",
    "        time_features = np.array([time_event, time_alert], dtype=np.float32)\n",
    "        \n",
    "        return time_features\n",
    "    \n",
    "    def apply_transforms(self, clip):\n",
    "        \"\"\"\n",
    "        Apply transforms to clip frames.\n",
    "        \"\"\"\n",
    "        if self.transform:\n",
    "            transformed_clip = []\n",
    "            for frame in clip:\n",
    "                # Convert NumPy array to PIL Image if it's not already\n",
    "                if isinstance(frame, np.ndarray):\n",
    "                    # Ensure frame has proper shape for transform\n",
    "                    if frame.shape[2] == 3:  # If it's (H, W, C)\n",
    "                        transformed_frame = self.transform(frame)\n",
    "                    else:\n",
    "                        # If somehow we got (C, H, W), rearrange it\n",
    "                        frame = np.transpose(frame, (1, 2, 0))\n",
    "                        transformed_frame = self.transform(frame)\n",
    "                else:\n",
    "                    transformed_frame = self.transform(frame)\n",
    "                \n",
    "                # Now transformed_frame is a tensor (C, H, W)\n",
    "                transformed_clip.append(transformed_frame)\n",
    "            \n",
    "            # Stack tensors to (T, C, H, W)\n",
    "            return torch.stack(transformed_clip).numpy()\n",
    "        return clip\n",
    "    \n",
    "    def time_to_frame(self, time, total_frames):\n",
    "        \"\"\"\n",
    "        Convert time to frame index.\n",
    "        \"\"\"\n",
    "        # Assuming time is normalized between 0 and 1\n",
    "        return int(time * (total_frames - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb5168bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms for video frames\n",
    "def get_transforms(mode='train'):\n",
    "    if mode == 'train':\n",
    "        return transforms.Compose([\n",
    "            # Don't include ToPILImage since our frames are already numpy arrays\n",
    "            transforms.ToPILImage(),  # Convert numpy array to PIL Image\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            # Don't include ToPILImage since our frames are already numpy arrays\n",
    "            transforms.ToPILImage(),  # Convert numpy array to PIL Image\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b58ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"\n",
    "    Get the device to use for training.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8919e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model with debug prints\n",
    "def train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.001):\n",
    "    # Check if GPU is available\n",
    "    device = get_device()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Debug: Print model architecture\n",
    "    print(f\"Model architecture:\\n{model}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Debug: Print batch information for first epoch\n",
    "        for batch_idx, (clips, time_features, targets) in enumerate(train_loader):\n",
    "            # Debug info for first batch\n",
    "            if epoch == 0 and batch_idx == 0:\n",
    "                print(f\"Input shapes - clips: {clips.shape}, time_features: {time_features.shape}, targets: {targets.shape}\")\n",
    "            \n",
    "            # Check if tensor dimensions need to be fixed\n",
    "            if clips.dim() == 5 and clips.shape[1] > 5:  # If second dim is too large, likely wrong order\n",
    "                # Permute to the correct order [B, C, T, H, W]\n",
    "                clips = clips.permute(0, 3, 2, 1, 4)\n",
    "                print(f\"Corrected clips shape: {clips.shape}\")\n",
    "            \n",
    "            clips = clips.to(device)\n",
    "            time_features = time_features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            try:\n",
    "                # Forward pass with error handling\n",
    "                outputs = model(clips, time_features)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.detach().item() * clips.size(0)\n",
    "                \n",
    "                # Debug info for first few batches\n",
    "                if epoch == 0 and batch_idx < 3:\n",
    "                    print(f\"Batch {batch_idx} processed successfully. Loss: {loss.detach().item():.4f}\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {batch_idx}:\")\n",
    "                print(f\"Clips shape: {clips.shape}\")\n",
    "                print(f\"Time features shape: {time_features.shape}\")\n",
    "                print(f\"Targets shape: {targets.shape}\")\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                raise  # Re-raise the exception to stop training\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_outputs = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for clips, time_features, targets in val_loader:\n",
    "                # Fix tensor dimensions if needed\n",
    "                if clips.dim() == 5 and clips.shape[1] > 5:\n",
    "                    clips = clips.permute(0, 3, 2, 1, 4)\n",
    "                \n",
    "                clips = clips.to(device)\n",
    "                time_features = time_features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                try:\n",
    "                    outputs = model(clips, time_features)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    val_loss += loss.item() * clips.size(0)\n",
    "                    \n",
    "                    all_outputs.extend(outputs.cpu().numpy())\n",
    "                    all_targets.extend(targets.cpu().numpy())\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error during validation:\")\n",
    "                    print(f\"Clips shape: {clips.shape}\")\n",
    "                    print(f\"Error: {str(e)}\")\n",
    "                    raise\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        all_outputs = np.array(all_outputs).flatten()\n",
    "        all_targets = np.array(all_targets).flatten()\n",
    "        \n",
    "        binary_preds = (all_outputs > 0.5).astype(int)\n",
    "        accuracy = np.mean(binary_preds == all_targets)\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0477570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for real-time inference\n",
    "class CollisionDetector:\n",
    "    def __init__(self, model_path, clip_len=16, spatial_size=(112, 112)):\n",
    "\n",
    "\n",
    "        self.device = get_device()\n",
    "        self.clip_len = clip_len\n",
    "        self.spatial_size = spatial_size\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = CollisionPredictor3DCNN(\n",
    "            input_channels=3, \n",
    "            clip_len=clip_len, \n",
    "            height=spatial_size[0], \n",
    "            width=spatial_size[1]\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Load model weights\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize frame buffer\n",
    "        self.buffer = deque(maxlen=clip_len)\n",
    "        \n",
    "        # Initialize transform\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        # Resize frame\n",
    "        frame = cv2.resize(frame, self.spatial_size)\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transform\n",
    "        frame = self.transform(frame)\n",
    "        \n",
    "        # Add to buffer\n",
    "        self.buffer.append(frame.numpy())\n",
    "        \n",
    "        # If buffer is not full, return None\n",
    "        if len(self.buffer) < self.clip_len:\n",
    "            return None\n",
    "        \n",
    "        # Create clip tensor\n",
    "        clip = np.array(self.buffer)\n",
    "        # Buffer shape is (T, C, H, W), need (C, T, H, W) for 3D CNN\n",
    "        clip = np.transpose(clip, (1, 0, 2, 3))  # (T, C, H, W) -> (C, T, H, W)\n",
    "        clip = torch.from_numpy(clip).float().unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Move to device\n",
    "        clip = clip.to(self.device)\n",
    "        \n",
    "        # Since we don't have time features for real-time prediction,\n",
    "        # we'll use default values (-1, -1) to indicate no specific time info\n",
    "        time_features = torch.tensor([[-1.0, -1.0]], dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            collision_prob = self.model(clip, time_features)\n",
    "            \n",
    "        return collision_prob.item()\n",
    "    \n",
    "    def predict_from_video(self, video_path, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Process an entire video and return frames with collision probabilities.\n",
    "        Returns list of (frame, probability) pairs where probability > threshold.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "        \n",
    "        # Reset buffer\n",
    "        self.buffer = deque(maxlen=self.clip_len)\n",
    "        \n",
    "        results = []\n",
    "        frame_idx = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            prob = self.process_frame(frame)\n",
    "            \n",
    "            if prob is not None and prob > threshold:\n",
    "                results.append((frame_idx, prob))\n",
    "            \n",
    "            frame_idx += 1\n",
    "        \n",
    "        cap.release()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b42905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "    \n",
    "# Parameters\n",
    "clip_len = 16\n",
    "spatial_size = (112, 112)\n",
    "batch_size = 16\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9aab832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d83e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory\n",
    "data_dir = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "45604dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{data_dir}/nexar-collision-prediction/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fdd69d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time_of_event</th>\n",
       "      <th>time_of_alert</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>822</td>\n",
       "      <td>19.5</td>\n",
       "      <td>18.633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>208</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  time_of_event  time_of_alert  target\n",
       "0  1924            NaN            NaN       0\n",
       "1   822           19.5         18.633       1\n",
       "2  1429            NaN            NaN       0\n",
       "3   208           19.8         19.233       1\n",
       "4  1904            NaN            NaN       0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1842c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = f'{data_dir}/nexar-collision-prediction/train' \n",
    "video_paths = []\n",
    "time_of_events = []\n",
    "time_of_alerts = []\n",
    "targets = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    video_id = str(int(row['id']))\n",
    "    id_len = len(video_id)\n",
    "    if id_len < 5:\n",
    "        video_id = '0' * (5 - id_len) + video_id\n",
    "\n",
    "    video_path = f\"{video_folder}/{video_id}.mp4\"\n",
    "    video_paths.append(video_path)\n",
    "    \n",
    "    # Convert time_of_event and time_of_alert to float\n",
    "    time_of_event = row['time_of_event'] if pd.notna(row['time_of_event']) else None\n",
    "    time_of_alert = row['time_of_alert'] if pd.notna(row['time_of_alert']) else None\n",
    "    \n",
    "    time_of_events.append(time_of_event)\n",
    "    time_of_alerts.append(time_of_alert)\n",
    "    \n",
    "    targets.append(row['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c6e3c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "# Split data into train and validation sets (80/20 split)\n",
    "split_idx = int(0.8 * len(video_paths))\n",
    "    \n",
    "train_transform = get_transforms(mode='train')\n",
    "val_transform = get_transforms(mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f4d8f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoCollisionDataset(\n",
    "    video_paths=video_paths[:split_idx],\n",
    "    time_of_events=time_of_events[:split_idx],\n",
    "    time_of_alerts=time_of_alerts[:split_idx],\n",
    "    targets=targets[:split_idx],\n",
    "    clip_len=clip_len,\n",
    "    spatial_size=spatial_size,\n",
    "    transform=train_transform\n",
    ")\n",
    "    \n",
    "val_dataset = VideoCollisionDataset(\n",
    "    video_paths=video_paths[split_idx:],\n",
    "    time_of_events=time_of_events[split_idx:],\n",
    "    time_of_alerts=time_of_alerts[split_idx:],\n",
    "    targets=targets[split_idx:],\n",
    "    clip_len=clip_len,\n",
    "    spatial_size=spatial_size,\n",
    "    transform=val_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c096cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "    \n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "13f11478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with input_channels=3, clip_len=16, height=112, width=112\n",
      "Calculated flat size: 100352 (512 * 4 * 7 * 7)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = CollisionPredictor3DCNN(\n",
    "    input_channels=3,\n",
    "    clip_len=clip_len,\n",
    "    height=spatial_size[0],\n",
    "    width=spatial_size[1]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f1093ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "CollisionPredictor3DCNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv3d(3, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=100352, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (time_fc): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=1088, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Input shapes - clips: torch.Size([16, 3, 16, 112, 112]), time_features: torch.Size([16, 2]), targets: torch.Size([16, 1])\n",
      "Batch 0 processed successfully. Loss: 0.7465\n",
      "Batch 1 processed successfully. Loss: 31.2500\n",
      "Batch 2 processed successfully. Loss: 50.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[39m\n\u001b[32m     20\u001b[39m train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Debug: Print batch information for first epoch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mclips\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Debug info for first batch\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mInput shapes - clips: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclips\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m, time_features: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtime_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m, targets: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/deepl/nexar/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:735\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    733\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    734\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    738\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    741\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/deepl/nexar/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:791\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    790\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    793\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/deepl/nexar/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mVideoCollisionDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Load video\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     video = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvideo_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Extract clip from video\u001b[39;00m\n\u001b[32m     21\u001b[39m     clip = \u001b[38;5;28mself\u001b[39m.extract_clip(video, idx)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mVideoCollisionDataset.load_video\u001b[39m\u001b[34m(self, video_path)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not open video file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     ret, frame = \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     62\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
